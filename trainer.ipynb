{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from libs.utils import try_gpu, QuantileLoss\n",
    "from libs.tft_model import TFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tick = pd.read_hdf('../data/data_tick_2months.h5', key='tick_data')\n",
    "data_stock = data_tick.xs('688303.XSHG', level='order_book_id') #! 取第x只股票\n",
    "date_list = data_stock.trading_date.unique() # 取出所有交易日的date\n",
    "\n",
    "df_tick_test = data_stock[data_stock['trading_date'] == date_list[-1]]  # 取最后一天数据为测试集\n",
    "df_tick_train = data_stock[data_stock['trading_date'] != date_list[-1]] # 其他交易日数据为测试集\n",
    "\n",
    "data_day = pd.read_hdf('../data/data_20days.h5', key='day_data')\n",
    "data_stock = data_day.xs('688303.XSHG', level='order_book_id') #! 取第x只股票\n",
    "day_list = data_stock.index.unique() # 取出所有交易日的date\n",
    "\n",
    "# df_day_test = data_stock[(data_stock['date'] >= day_list[-21]) & (data_stock['date'] < day_list[-1])]  \n",
    "# df_day_train = data_stock[data_stock['date'] < day_list[-2]] \n",
    "df_day_test = data_stock.loc[day_list[-21]: day_list[-1]]\n",
    "df_day_train = data_stock.loc[: day_list[-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resample tick data in 3s pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tick_list = []\n",
    "for i in range(len(date_list)-1):\n",
    "    data_piece = df_tick_train[df_tick_train['trading_date'] == date_list[i]]\n",
    "    data_piece = data_piece.resample('3S', origin=f'{date_list[i].date()} 09:30:00').ffill()\n",
    "    locs1 = data_piece.index.indexer_between_time('09:30:00', '11:30:00', include_start=True,include_end=True)\n",
    "    locs2 = data_piece.index.indexer_between_time('13:00:00', '15:00:00', include_start=True,include_end=True)\n",
    "    data_piece = pd.concat((data_piece.iloc[locs1], data_piece.iloc[locs2]), axis=0)\n",
    "    data_piece[['volume', 'total_turnover', 'num_trades']] = data_piece[['volume', 'total_turnover', 'num_trades']].diff().bfill()\n",
    "    df_tick_list.append(data_piece)\n",
    "df_tick_train = pd.concat(df_tick_list, axis=0)\n",
    "\n",
    "df_tick_test = df_tick_test.resample('3S', origin=f'{date_list[-1].date()} 09:30:00').ffill()\n",
    "locs1 = df_tick_test.index.indexer_between_time('09:30:00', '11:30:00', include_start=True,include_end=True)\n",
    "locs2 = df_tick_test.index.indexer_between_time('13:00:00', '15:00:00', include_start=True,include_end=True)\n",
    "df_tick_test = pd.concat((df_tick_test.iloc[locs1], df_tick_test.iloc[locs2]), axis=0)\n",
    "df_tick_test[['volume', 'total_turnover', 'num_trades']] = df_tick_test[['volume', 'total_turnover', 'num_trades']].diff().bfill()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_train(Dataset):\n",
    "    def __init__(self, df_tick_train, df_day_train):\n",
    "        #self.time_steps = time_steps\n",
    "        self.df_tick = df_tick_train\n",
    "        self.date_list = df_tick_train.trading_date.unique()\n",
    "        self.df_day = df_day_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        day_num = index // 4802\n",
    "\n",
    "        static_feats_numeric = self.df_day.iloc[day_num: day_num+20, :]\n",
    "        static_feats_numeric = static_feats_numeric.values.flatten()\n",
    "        static_feats_numeric = torch.tensor(static_feats_numeric, dtype=torch.float32)\n",
    "\n",
    "        historical_ts_numeric = self.df_tick[self.df_tick['trading_date'] == self.date_list[day_num]]\n",
    "        historical_ts_numeric = historical_ts_numeric.drop(columns=['trading_date'])\n",
    "        num = index % (4802 - 5*20*3 - 20*3 + 1)\n",
    "        label = (historical_ts_numeric.iloc[num+5*20*3+20*3-1]['last'] - historical_ts_numeric.iloc[num+5*20*3]['last'])/historical_ts_numeric.iloc[num+5*20*3]['last']\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        historical_ts_numeric = historical_ts_numeric.iloc[num:num+5*20*3, :]\n",
    "        historical_ts_numeric = torch.tensor(historical_ts_numeric.values, dtype=torch.float32)\n",
    "        \n",
    "        return static_feats_numeric, historical_ts_numeric, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.date_list) * (4802 - 5*20*3 - 20*3 + 1)\n",
    "\n",
    "\n",
    "def LoadData_train(df_tick_train, df_day_train, batch_size, shuffle=False, num_workers=0):\n",
    "    dataset = Dataset_train(df_tick_train, df_day_train)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_test(Dataset):\n",
    "    def __init__(self, df_tick_test, df_day_test):\n",
    "        # self.time_steps = time_steps\n",
    "        self.df_tick = df_tick_test\n",
    "        self.df_day = df_day_test       \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        static_feats_numeric = self.df_day.values.flatten()\n",
    "        static_feats_numeric = torch.tensor(static_feats_numeric, dtype=torch.float32)\n",
    "\n",
    "        historical_ts_numeric = self.df_tick\n",
    "        label = (historical_ts_numeric.iloc[index+5*20*3+20*3-1]['last'] - historical_ts_numeric.iloc[index+5*20*3]['last'])/historical_ts_numeric.iloc[index+5*20*3]['last']\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        historical_ts_numeric = historical_ts_numeric.iloc[index:index+5*20*3, :]\n",
    "        historical_ts_numeric = torch.tensor(historical_ts_numeric.values, dtype=torch.float32)\n",
    "\n",
    "        return static_feats_numeric, historical_ts_numeric, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df_tick.shape[0] - 5*20*3 - 20*3 + 1\n",
    "\n",
    "\n",
    "def LoadData_test(df_tick_test, df_day_test, batch_size, shuffle=False, num_workers=0):\n",
    "    dataset = Dataset_test(df_tick_test, df_day_test)\n",
    "    return DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"data_props\": {\n",
    "        \"num_historical_numeric\": 31,\n",
    "        \"num_historical_categorical\": 0,\n",
    "        \"historical_categorical_cardinalities\": [],\n",
    "        \"num_static_numeric\": 200,\n",
    "        \"num_static_categorical\": 0,\n",
    "        \"static_categorical_cardinalities\": [],\n",
    "        #\"num_future_numeric\": 0,\n",
    "        #\"num_future_categorical\": 0,\n",
    "        #\"future_categorical_cardinalities\": [],\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"attention_heads\": 1,\n",
    "        \"dropout\": 0.3,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"output_quantiles\": [0.5], # [0.1, 0.5, 0.9],\n",
    "        \"state_size\": 256\n",
    "    },\n",
    "    \"task_type\": 'regression',\n",
    "    \"target_window_start\": None,\n",
    "}\n",
    "\n",
    "model = TFT(config)\n",
    "device = try_gpu(i=0)\n",
    "model.to(device)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 30\n",
    "batch_size = 64 * 3\n",
    "data_iter = LoadData_train(df_tick_train, df_day_train, batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [28:32<13:47:39, 1712.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [30:46<14:52:32, 1846.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\01-成员\\宋强\\Quant\\TFT-pytorch\\trainer.ipynb 单元格 11\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/01-%E6%88%90%E5%91%98/%E5%AE%8B%E5%BC%BA/Quant/TFT-pytorch/trainer.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mpredicted_quantiles\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/01-%E6%88%90%E5%91%98/%E5%AE%8B%E5%BC%BA/Quant/TFT-pytorch/trainer.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m l \u001b[39m=\u001b[39m loss(output, label)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/01-%E6%88%90%E5%91%98/%E5%AE%8B%E5%BC%BA/Quant/TFT-pytorch/trainer.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m l\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/01-%E6%88%90%E5%91%98/%E5%AE%8B%E5%BC%BA/Quant/TFT-pytorch/trainer.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/01-%E6%88%90%E5%91%98/%E5%AE%8B%E5%BC%BA/Quant/TFT-pytorch/trainer.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m l\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\Anaconda\\Anaconda\\envs\\Quant\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\Anaconda\\Anaconda\\envs\\Quant\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    for static_feats_numeric, historical_ts_numeric, label in data_iter:\n",
    "        batch = {\n",
    "            'static_feats_numeric': static_feats_numeric,  # 静态数值特征，形状：[num_samples x num_static_numeric]\n",
    "            'historical_ts_numeric': historical_ts_numeric,  # 历史数值时间序列，形状：[num_samples x num_historical_steps x num_historical_numeric]\n",
    "            'static_feats_categorical': torch.empty(1),\n",
    "            'historical_ts_categorical': torch.empty(1),\n",
    "        }\n",
    "        for k in batch.keys():\n",
    "            batch[k] = batch[k].to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        output = output['predicted_quantiles']\n",
    "\n",
    "        l = loss(output, label)\n",
    "\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += l.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {(running_loss / len(data_iter)):.16f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model / Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
